{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trust Region Policy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Principle:** Trust Region Policy Updates\n\n",
    "**Definition:** Constrains policy updates within a Kullback-Leibler Divergence trust region for monotonic improvement (TRPO).\n\n",
    "**Algorithm Description:** Trust Region Policy Optimization constrains policy updates to a trust region defined by KL divergence between old and new policies. It uses conjugate gradient methods to find the largest policy improvement step within this trust region, guaranteeing monotonic performance improvement.\n\n",
    "**Typical Use Cases:**\n- Avoids catastrophic performance drops\n- Making stable and monotonic policy updates\n- More complex than ppo.\n- When guarantees on update step size are needed\n- Works with continuous and discrete action spaces\n\n",
    "**Assumptions:**\n- Computationally intensive\n- Continuous/discrete actions\n- On-policy learning\n- Stable updates\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Import Libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. TRPO Algorithm\n",
    "\n",
    "TRPO (Trust Region Policy Optimization) constrains policy updates:\n",
    "\n",
    "**Maximize:** E[\u03c0_\u03b8(a|s)/\u03c0_\u03b8_old(a|s) * A(s,a)]\n",
    "\n",
    "**Subject to:** KL(\u03c0_\u03b8_old || \u03c0_\u03b8) \u2264 \u03b4\n",
    "\n",
    "Where:\n",
    "- KL = Kullback-Leibler divergence\n",
    "- \u03b4 = trust region constraint (e.g., 0.01)\n",
    "- A = advantage function\n",
    "\n",
    "**Simplified implementation using KL penalty instead of hard constraint**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class TRPOAgent:\n",
    "    \"\"\"Simplified TRPO with KL penalty\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99, kl_target=0.01):\n",
    "        self.gamma = gamma\n",
    "        self.kl_target = kl_target\n",
    "        self.beta = 1.0  # KL penalty coefficient\n",
    "        \n",
    "        self.policy = PolicyNetwork(state_dim, action_dim)\n",
    "        self.value = ValueNetwork(state_dim)\n",
    "        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.value_optimizer = optim.Adam(self.value.parameters(), lr=lr)\n",
    "        \n",
    "        self.buffer = {'states': [], 'actions': [], 'rewards': [], 'dones': []}\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            state_t = torch.FloatTensor(state)\n",
    "            probs = self.policy(state_t)\n",
    "            dist = Categorical(probs)\n",
    "            action = dist.sample()\n",
    "        return action.item()\n",
    "    \n",
    "    def compute_kl(self, old_probs, new_probs):\n",
    "        \"\"\"Compute KL divergence\"\"\"\n",
    "        return (old_probs * (torch.log(old_probs + 1e-10) - torch.log(new_probs + 1e-10))).sum(-1).mean()\n",
    "    \n",
    "    def update(self):\n",
    "        states = torch.FloatTensor(self.buffer['states'])\n",
    "        actions = torch.LongTensor(self.buffer['actions'])\n",
    "        rewards = self.buffer['rewards']\n",
    "        dones = self.buffer['dones']\n",
    "        \n",
    "        # Compute returns\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r, done in zip(reversed(rewards), reversed(dones)):\n",
    "            if done:\n",
    "                G = 0\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        returns = torch.FloatTensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-7)\n",
    "        \n",
    "        # Update value function\n",
    "        values = self.value(states).squeeze()\n",
    "        value_loss = F.mse_loss(values, returns)\n",
    "        self.value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        self.value_optimizer.step()\n",
    "        \n",
    "        # Compute advantages\n",
    "        with torch.no_grad():\n",
    "            advantages = returns - self.value(states).squeeze()\n",
    "        \n",
    "        # Old policy distribution\n",
    "        with torch.no_grad():\n",
    "            old_probs = self.policy(states)\n",
    "        \n",
    "        # Update policy with KL constraint (penalty method)\n",
    "        new_probs = self.policy(states)\n",
    "        dist = Categorical(new_probs)\n",
    "        log_probs = dist.log_prob(actions)\n",
    "        \n",
    "        policy_loss = -(log_probs * advantages).mean()\n",
    "        kl_div = self.compute_kl(old_probs, new_probs)\n",
    "        total_loss = policy_loss + self.beta * kl_div\n",
    "        \n",
    "        self.policy_optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        self.policy_optimizer.step()\n",
    "        \n",
    "        # Adjust KL penalty\n",
    "        if kl_div > 1.5 * self.kl_target:\n",
    "            self.beta *= 2\n",
    "        elif kl_div < self.kl_target / 1.5:\n",
    "            self.beta *= 0.5\n",
    "        \n",
    "        # Clear buffer\n",
    "        self.buffer = {'states': [], 'actions': [], 'rewards': [], 'dones': []}"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Train TRPO Agent"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "agent = TRPOAgent(env.observation_space.shape[0], env.action_space.n)\n",
    "\n",
    "episode_rewards = []\n",
    "update_interval = 20\n",
    "\n",
    "print('Training TRPO...')\n",
    "for episode in range(500):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for t in range(500):\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        \n",
    "        agent.buffer['states'].append(state)\n",
    "        agent.buffer['actions'].append(action)\n",
    "        agent.buffer['rewards'].append(reward)\n",
    "        agent.buffer['dones'].append(terminated or truncated)\n",
    "        \n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    episode_rewards.append(total_reward)\n",
    "    \n",
    "    if (episode + 1) % update_interval == 0:\n",
    "        agent.update()\n",
    "    \n",
    "    if (episode + 1) % 50 == 0:\n",
    "        print(f'Episode {episode+1}, Avg: {np.mean(episode_rewards[-50:]):.2f}, Beta: {agent.beta:.3f}')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Visualize Results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(episode_rewards, alpha=0.3, label='Raw')\n",
    "if len(episode_rewards) > 20:\n",
    "    ma = np.convolve(episode_rewards, np.ones(20)/20, mode='valid')\n",
    "    plt.plot(ma, linewidth=2, label='MA(20)')\n",
    "plt.axhline(195, color='r', linestyle='--', label='Solved')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('TRPO Training Performance')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nFinal Avg Reward: {np.mean(episode_rewards[-100:]):.2f}')\n",
    "env.close()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}