{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proximal Policy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Principle:** Constrained Policy Optimization\n\n",
    "**Definition:** Maximizes surrogate objective while clipping probability ratios to ensure stable updates (PPO).\n\n",
    "**Algorithm Description:** Proximal Policy Optimization clips policy updates to prevent excessively large changes, ensuring new policies don't deviate too far from old ones. This clipping mechanism provides a simpler alternative to trust region methods while maintaining stable, reliable learning across various environments and action spaces.\n\n",
    "**Typical Use Cases:**\n- Balances sample efficiency and ease of tuning.\n- Robust, general-purpose policy gradient algorithm\n- Simpler to implement than trpo\n- When stability and reliable performance are needed\n- Works with continuous and discrete action spaces\n\n",
    "**Assumptions:**\n- Continuous/discrete actions\n- Large datasets\n- On-policy learning\n- Stable updates\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### âš™ï¸ Prerequisites & Setup\n\n**Required Packages:**\n- `gymnasium>=0.29.0` - RL environment\n- `torch>=2.0.0` - Deep learning framework\n- `numpy`, `matplotlib`, `seaborn` - Data processing and visualization\n\n**Installation:**\n```bash\npip install gymnasium torch numpy matplotlib seaborn\n```\n\n**â±ï¸ Estimated Training Time:**\n- CPU: ~10-15 minutes\n- GPU: ~4-6 minutes\n\n**ðŸ’¡ Quick Test:** To reduce training time, change `n_episodes=500` to `n_episodes=100` in the training cell.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Import Libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "torch.manual_seed(42)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. PPO Algorithm\n",
    "\n",
    "PPO (Proximal Policy Optimization) uses a clipped surrogate objective:\n",
    "\n",
    "**L^CLIP(Î¸) = E[min(r_t(Î¸)A_t, clip(r_t(Î¸), 1-Îµ, 1+Îµ)A_t)]**\n",
    "\n",
    "Where:\n",
    "- r_t(Î¸) = Ï€_Î¸(a|s) / Ï€_Î¸_old(a|s) (probability ratio)\n",
    "- A_t = advantage estimate\n",
    "- Îµ = clip parameter (typically 0.2)\n",
    "\n",
    "**Key advantage:** Prevents large policy updates"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class ActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Linear(hidden_dim, action_dim)\n",
    "        self.critic = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = self.shared(state)\n",
    "        return F.softmax(self.actor(x), dim=-1), self.critic(x)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class PPOAgent:\n    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99, \n                 eps_clip=0.2, K_epochs=4):\n        self.gamma = gamma\n        self.eps_clip = eps_clip\n        self.K_epochs = K_epochs\n        \n        self.policy = ActorCriticNetwork(state_dim, action_dim)\n        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n        self.policy_old = ActorCriticNetwork(state_dim, action_dim)\n        self.policy_old.load_state_dict(self.policy.state_dict())\n        \n        self.buffer = {'states': [], 'actions': [], 'rewards': [], \n                      'log_probs': [], 'dones': []}\n    \n    def select_action(self, state):\n        with torch.no_grad():\n            state = torch.FloatTensor(state)\n            probs, _ = self.policy_old(state)\n            dist = Categorical(probs)\n            action = dist.sample()\n            log_prob = dist.log_prob(action)\n        \n        self.buffer['states'].append(state)\n        self.buffer['actions'].append(action)\n        self.buffer['log_probs'].append(log_prob)\n        return action.item()\n    \n    def update(self):\n        # Calculate returns\n        returns = []\n        G = 0\n        for r, done in zip(reversed(self.buffer['rewards']), \n                          reversed(self.buffer['dones'])):\n            if done:\n                G = 0\n            G = r + self.gamma * G\n            returns.insert(0, G)\n        \n        returns = torch.tensor(returns)\n        returns = (returns - returns.mean()) / (returns.std() + 1e-7)\n        \n        old_states = torch.stack(self.buffer['states'])\n        old_actions = torch.stack(self.buffer['actions'])\n        old_log_probs = torch.stack(self.buffer['log_probs'])\n        \n        # PPO update for K epochs\n        for _ in range(self.K_epochs):\n            probs, state_values = self.policy(old_states)\n            dist = Categorical(probs)\n            log_probs = dist.log_prob(old_actions)\n            entropy = dist.entropy()\n            \n            # Calculate ratio and clipped objective\n            ratios = torch.exp(log_probs - old_log_probs.detach())\n            advantages = returns - state_values.squeeze().detach()\n            \n            surr1 = ratios * advantages\n            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n            \n            loss = -torch.min(surr1, surr2).mean() + \\\n                   0.5 * F.mse_loss(state_values.squeeze(), returns) - \\\n                   0.01 * entropy.mean()\n            \n            self.optimizer.zero_grad()\n            loss.backward()\n            \n            # Gradient clipping for training stability\n            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), max_norm=0.5)\n            \n            self.optimizer.step()\n        \n        # Update old policy\n        self.policy_old.load_state_dict(self.policy.state_dict())\n        \n        # Clear buffer\n        self.buffer = {'states': [], 'actions': [], 'rewards': [], \n                      'log_probs': [], 'dones': []}",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Train PPO Agent"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "agent = PPOAgent(env.observation_space.shape[0], env.action_space.n)\n",
    "\n",
    "episode_rewards = []\n",
    "update_interval = 20\n",
    "\n",
    "print('Training PPO...')\n",
    "for episode in range(500):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for t in range(500):\n",
    "        action = agent.select_action(state)\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        agent.buffer['rewards'].append(reward)\n",
    "        agent.buffer['dones'].append(terminated or truncated)\n",
    "        total_reward += reward\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    episode_rewards.append(total_reward)\n",
    "    \n",
    "    if (episode + 1) % update_interval == 0:\n",
    "        agent.update()\n",
    "    \n",
    "    if (episode + 1) % 50 == 0:\n",
    "        print(f'Episode {episode+1}, Avg Reward: {np.mean(episode_rewards[-50:]):.2f}')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Visualize Results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(episode_rewards, alpha=0.3)\n",
    "if len(episode_rewards) > 20:\n",
    "    ma = np.convolve(episode_rewards, np.ones(20)/20, mode='valid')\n",
    "    plt.plot(ma, linewidth=2, label='MA(20)')\n",
    "plt.axhline(195, color='r', linestyle='--', label='Solved')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('PPO Training Performance')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f'Final performance: {np.mean(episode_rewards[-100:]):.2f}')\n",
    "env.close()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}