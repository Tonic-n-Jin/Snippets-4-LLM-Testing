{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Principle:** On-Policy Value Learning\n\n",
    "**Definition:** Updates action-value using the current policyÃ¢â‚¬â„¢s next action, ensuring on-policy consistency (SARSA).\n\n",
    "**Algorithm Description:** SARSA learns an action-value function Q(s,a) by updating based on the actual next action taken according to the current policy (State-Action-Reward-State-Action). Unlike Q-Learning's off-policy approach, SARSA evaluates and improves the same policy being followed, often resulting in more conservative learning.\n\n",
    "**Typical Use Cases:**\n- Can be safer than q-learning during exploration.\n- Finding optimal or near-optimal policy in model-free environments\n- Good for discrete state/action spaces\n- On-policy learning (evaluates the policy being followed)\n\n",
    "**Assumptions:**\n- Discrete state/action spaces\n- Model-free\n- On-policy learning\n- Unsuitable for large/continuous spaces\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### 1. Import Libraries",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gymnasium as gym\nfrom collections import defaultdict\nimport pandas as pd\n\n# Set plotting style\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (12, 6)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 2. SARSA Agent\n\nSARSA (State-Action-Reward-State-Action) is an on-policy, value-based RL algorithm that learns Q(s, a) using the actual next action:\n\n**Q(s, a) ← Q(s, a) + α [r + γ Q(s', a') - Q(s, a)]**\n\nKey difference from Q-Learning: Uses **actual next action a'** instead of max Q(s', a')\n\nWhere:\n- α = learning rate\n- γ = discount factor\n- r = reward\n- s' = next state\n- a' = actual next action (chosen by policy)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class SARSAAgent:\n    \"\"\"SARSA agent with epsilon-greedy exploration\"\"\"\n    \n    def __init__(self, n_states, n_actions, learning_rate=0.1, \n                 discount_factor=0.95, epsilon=1.0, epsilon_decay=0.995, \n                 epsilon_min=0.01):\n        \"\"\"\n        Initialize SARSA agent\n        \n        Parameters:\n        -----------\n        n_states : int\n            Number of states in the environment\n        n_actions : int\n            Number of actions available\n        learning_rate : float\n            Learning rate (alpha)\n        discount_factor : float\n            Discount factor (gamma)\n        epsilon : float\n            Initial exploration rate\n        epsilon_decay : float\n            Decay rate for epsilon\n        epsilon_min : float\n            Minimum epsilon value\n        \"\"\"\n        self.n_states = n_states\n        self.n_actions = n_actions\n        self.lr = learning_rate\n        self.gamma = discount_factor\n        self.epsilon = epsilon\n        self.epsilon_decay = epsilon_decay\n        self.epsilon_min = epsilon_min\n        \n        # Initialize Q-table with zeros\n        self.q_table = np.zeros((n_states, n_actions))\n        \n    def select_action(self, state):\n        \"\"\"Select action using epsilon-greedy policy\"\"\"\n        if np.random.random() < self.epsilon:\n            # Exploration: random action\n            return np.random.randint(self.n_actions)\n        else:\n            # Exploitation: best action from Q-table\n            return np.argmax(self.q_table[state])\n    \n    def update(self, state, action, reward, next_state, next_action, done):\n        \"\"\"\n        Update Q-value using SARSA update rule\n        \n        Key difference from Q-Learning: Uses actual next_action instead of max\n        \"\"\"\n        # Current Q-value\n        current_q = self.q_table[state, action]\n        \n        # Q-value for next state-action pair (on-policy)\n        next_q = 0 if done else self.q_table[next_state, next_action]\n        \n        # SARSA update: Q(s,a) ← Q(s,a) + α[r + γ Q(s',a') - Q(s,a)]\n        new_q = current_q + self.lr * (reward + self.gamma * next_q - current_q)\n        \n        self.q_table[state, action] = new_q\n    \n    def decay_epsilon(self):\n        \"\"\"Decay exploration rate\"\"\"\n        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 3. Train SARSA Agent\n\nNote: SARSA requires knowing the next action before updating, making it an on-policy algorithm.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def train_sarsa(env, agent, n_episodes=10000, max_steps=100):\n    \"\"\"\n    Train SARSA agent\n    \n    Parameters:\n    -----------\n    env : gym.Env\n        Environment to train on\n    agent : SARSAAgent\n        SARSA agent\n    n_episodes : int\n        Number of episodes to train\n    max_steps : int\n        Maximum steps per episode\n        \n    Returns:\n    --------\n    episode_rewards : list\n        Rewards obtained in each episode\n    episode_lengths : list\n        Length of each episode\n    \"\"\"\n    episode_rewards = []\n    episode_lengths = []\n    \n    for episode in range(n_episodes):\n        state, _ = env.reset()\n        total_reward = 0\n        \n        # Select initial action\n        action = agent.select_action(state)\n        \n        for step in range(max_steps):\n            # Take action and observe result\n            next_state, reward, terminated, truncated, _ = env.step(action)\n            done = terminated or truncated\n            \n            # Select next action (needed for SARSA update)\n            next_action = agent.select_action(next_state)\n            \n            # Update Q-table using SARSA (uses next_action, not max)\n            agent.update(state, action, reward, next_state, next_action, done)\n            \n            total_reward += reward\n            state = next_state\n            action = next_action  # Use the same action for next step (on-policy)\n            \n            if done:\n                break\n        \n        # Decay epsilon after each episode\n        agent.decay_epsilon()\n        \n        episode_rewards.append(total_reward)\n        episode_lengths.append(step + 1)\n        \n        # Print progress\n        if (episode + 1) % 1000 == 0:\n            avg_reward = np.mean(episode_rewards[-100:])\n            print(f\"Episode {episode + 1}/{n_episodes}, \"\n                  f\"Avg Reward (last 100): {avg_reward:.3f}, \"\n                  f\"Epsilon: {agent.epsilon:.3f}\")\n    \n    return episode_rewards, episode_lengths",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create FrozenLake environment\nenv = gym.make('FrozenLake-v1', is_slippery=True, render_mode=None)\n\n# Get environment dimensions\nn_states = env.observation_space.n\nn_actions = env.action_space.n\n\nprint(f\"Environment: FrozenLake-v1\")\nprint(f\"Number of states: {n_states}\")\nprint(f\"Number of actions: {n_actions}\")\n\n# Create SARSA agent\nagent = SARSAAgent(\n    n_states=n_states,\n    n_actions=n_actions,\n    learning_rate=0.1,\n    discount_factor=0.95,\n    epsilon=1.0,\n    epsilon_decay=0.995,\n    epsilon_min=0.01\n)\n\n# Train the agent\nprint(\"\\nTraining SARSA agent...\")\nepisode_rewards, episode_lengths = train_sarsa(\n    env, agent, n_episodes=10000, max_steps=100\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4. Visualize Training Results",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Calculate moving average for smoother visualization\ndef moving_average(data, window_size=100):\n    \"\"\"Calculate moving average\"\"\"\n    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n\n# Create subplots\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Plot 1: Episode Rewards\naxes[0, 0].plot(episode_rewards, alpha=0.3, label='Raw Rewards')\nif len(episode_rewards) > 100:\n    axes[0, 0].plot(moving_average(episode_rewards, 100), \n                    label='Moving Average (100 episodes)', linewidth=2)\naxes[0, 0].set_xlabel('Episode')\naxes[0, 0].set_ylabel('Total Reward')\naxes[0, 0].set_title('SARSA: Episode Rewards Over Time')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Plot 2: Episode Lengths\naxes[0, 1].plot(episode_lengths, alpha=0.3, label='Raw Lengths')\nif len(episode_lengths) > 100:\n    axes[0, 1].plot(moving_average(episode_lengths, 100), \n                    label='Moving Average (100 episodes)', linewidth=2)\naxes[0, 1].set_xlabel('Episode')\naxes[0, 1].set_ylabel('Episode Length')\naxes[0, 1].set_title('Episode Lengths Over Time')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Plot 3: Success Rate\nwindow = 100\nsuccess_rate = []\nfor i in range(len(episode_rewards) - window + 1):\n    success_rate.append(np.mean([r > 0 for r in episode_rewards[i:i+window]]))\naxes[1, 0].plot(success_rate, linewidth=2, color='green')\naxes[1, 0].set_xlabel('Episode')\naxes[1, 0].set_ylabel('Success Rate')\naxes[1, 0].set_title(f'Success Rate (Rolling {window} episodes)')\naxes[1, 0].grid(True, alpha=0.3)\n\n# Plot 4: Q-Table Heatmap\nim = axes[1, 1].imshow(agent.q_table, cmap='viridis', aspect='auto')\naxes[1, 1].set_xlabel('Action')\naxes[1, 1].set_ylabel('State')\naxes[1, 1].set_title('Learned Q-Table Values (SARSA)')\nplt.colorbar(im, ax=axes[1, 1])\n\nplt.tight_layout()\nplt.show()\n\n# Print final statistics\nprint(f\"\\nTraining Complete!\")\nprint(f\"Final epsilon: {agent.epsilon:.4f}\")\nprint(f\"Average reward (last 100 episodes): {np.mean(episode_rewards[-100:]):.4f}\")\nprint(f\"Success rate (last 100 episodes): {np.mean([r > 0 for r in episode_rewards[-100:]]):.2%}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 5. Evaluate Learned Policy",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def evaluate_policy(env, agent, n_episodes=100, max_steps=100):\n    \"\"\"Evaluate the learned policy\"\"\"\n    rewards = []\n    lengths = []\n    successes = []\n    \n    # Save current epsilon and set to 0 for pure exploitation\n    original_epsilon = agent.epsilon\n    agent.epsilon = 0.0\n    \n    for episode in range(n_episodes):\n        state, _ = env.reset()\n        total_reward = 0\n        \n        for step in range(max_steps):\n            action = agent.select_action(state)\n            next_state, reward, terminated, truncated, _ = env.step(action)\n            done = terminated or truncated\n            \n            total_reward += reward\n            state = next_state\n            \n            if done:\n                break\n        \n        rewards.append(total_reward)\n        lengths.append(step + 1)\n        successes.append(total_reward > 0)\n    \n    # Restore original epsilon\n    agent.epsilon = original_epsilon\n    \n    results = {\n        'mean_reward': np.mean(rewards),\n        'std_reward': np.std(rewards),\n        'mean_length': np.mean(lengths),\n        'success_rate': np.mean(successes)\n    }\n    \n    return results\n\n# Evaluate the trained agent\neval_results = evaluate_policy(env, agent, n_episodes=100)\n\nprint(\"=\" * 50)\nprint(\"SARSA EVALUATION RESULTS\")\nprint(\"=\" * 50)\nprint(f\"Mean Reward: {eval_results['mean_reward']:.4f} ± {eval_results['std_reward']:.4f}\")\nprint(f\"Mean Episode Length: {eval_results['mean_length']:.2f}\")\nprint(f\"Success Rate: {eval_results['success_rate']:.2%}\")\nprint(\"=\" * 50)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 6. Visualize Learned Policy\n\nDisplay the optimal action for each state and state values.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Extract optimal policy from Q-table\noptimal_policy = np.argmax(agent.q_table, axis=1)\naction_symbols = {0: '←', 1: '↓', 2: '→', 3: '↑'}\n\n# Create policy visualization for 4x4 grid\ngrid_size = 4\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Plot 1: Policy as arrows\nfor i in range(grid_size):\n    for j in range(grid_size):\n        state = i * grid_size + j\n        action = optimal_policy[state]\n        axes[0].text(j, i, action_symbols[action], \n                    ha='center', va='center', fontsize=20)\naxes[0].set_xlim(-0.5, grid_size - 0.5)\naxes[0].set_ylim(-0.5, grid_size - 0.5)\naxes[0].set_xticks(range(grid_size))\naxes[0].set_yticks(range(grid_size))\naxes[0].grid(True)\naxes[0].invert_yaxis()\naxes[0].set_title('SARSA Learned Policy (Optimal Actions)', fontsize=14, fontweight='bold')\naxes[0].set_xlabel('Column')\naxes[0].set_ylabel('Row')\n\n# Plot 2: State Values\nstate_values = np.max(agent.q_table, axis=1).reshape(grid_size, grid_size)\nim = axes[1].imshow(state_values, cmap='RdYlGn', aspect='auto')\naxes[1].set_title('State Values (Max Q-value)', fontsize=14, fontweight='bold')\naxes[1].set_xlabel('Column')\naxes[1].set_ylabel('Row')\naxes[1].set_xticks(range(grid_size))\naxes[1].set_yticks(range(grid_size))\n\nfor i in range(grid_size):\n    for j in range(grid_size):\n        text = axes[1].text(j, i, f'{state_values[i, j]:.3f}',\n                           ha='center', va='center', color='black', fontsize=10)\n\nplt.colorbar(im, ax=axes[1])\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nSARSA Policy Summary:\")\nprint(\"=\" * 50)\nprint(f\"Algorithm: SARSA (On-Policy TD Control)\")\nprint(f\"Total states: {n_states}\")\nprint(f\"Actions: {['LEFT', 'DOWN', 'RIGHT', 'UP']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}