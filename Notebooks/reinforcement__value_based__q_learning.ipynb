{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Principle:** Value Function Learning\n\n",
    "**Definition:** Learns the expected return from each state-action pair using off-policy temporal difference (Q-Learning).\n\n",
    "**Algorithm Description:** Q-Learning learns an action-value function Q(s,a) that estimates the expected cumulative reward of taking action a in state s and following the optimal policy thereafter. It updates Q-values using the Bellman equation and observed rewards, learning the optimal policy without requiring a model of the environment.\n\n",
    "**Typical Use Cases:**\n- Classic algorithm for discrete state/action spaces\n- Finding optimal policy in model-free environments\n- Off-policy learning (learns optimal policy while exploring)\n- Simple control problems (e.g., grid worlds).\n\n",
    "**Assumptions:**\n- Discrete state/action spaces\n- Model-free\n- Off-policy learning\n- Unsuitable for large/continuous spaces\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### 1. Import Libraries",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gymnasium as gym\nfrom collections import defaultdict\nimport pandas as pd\n\n# Set plotting style\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (12, 6)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 2. Q-Learning Agent\n\nQ-Learning is an off-policy, value-based RL algorithm that learns the optimal action-value function Q(s, a) using the Bellman equation:\n\n**Q(s, a) ← Q(s, a) + α [r + γ max Q(s', a') - Q(s, a)]**\n\nWhere:\n- α = learning rate\n- γ = discount factor\n- r = reward\n- s' = next state",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class QLearningAgent:\n    \"\"\"Q-Learning agent with epsilon-greedy exploration\"\"\"\n    \n    def __init__(self, n_states, n_actions, learning_rate=0.1, \n                 discount_factor=0.95, epsilon=1.0, epsilon_decay=0.995, \n                 epsilon_min=0.01):\n        \"\"\"\n        Initialize Q-Learning agent\n        \n        Parameters:\n        -----------\n        n_states : int\n            Number of states in the environment\n        n_actions : int\n            Number of actions available\n        learning_rate : float\n            Learning rate (alpha)\n        discount_factor : float\n            Discount factor (gamma)\n        epsilon : float\n            Initial exploration rate\n        epsilon_decay : float\n            Decay rate for epsilon\n        epsilon_min : float\n            Minimum epsilon value\n        \"\"\"\n        self.n_states = n_states\n        self.n_actions = n_actions\n        self.lr = learning_rate\n        self.gamma = discount_factor\n        self.epsilon = epsilon\n        self.epsilon_decay = epsilon_decay\n        self.epsilon_min = epsilon_min\n        \n        # Initialize Q-table with zeros\n        self.q_table = np.zeros((n_states, n_actions))\n        \n    def select_action(self, state):\n        \"\"\"Select action using epsilon-greedy policy\"\"\"\n        if np.random.random() < self.epsilon:\n            # Exploration: random action\n            return np.random.randint(self.n_actions)\n        else:\n            # Exploitation: best action from Q-table\n            return np.argmax(self.q_table[state])\n    \n    def update(self, state, action, reward, next_state, done):\n        \"\"\"Update Q-value using Q-Learning update rule\"\"\"\n        # Current Q-value\n        current_q = self.q_table[state, action]\n        \n        # Maximum Q-value for next state (off-policy)\n        max_next_q = 0 if done else np.max(self.q_table[next_state])\n        \n        # Q-Learning update: Q(s,a) ← Q(s,a) + α[r + γ max Q(s',a') - Q(s,a)]\n        new_q = current_q + self.lr * (reward + self.gamma * max_next_q - current_q)\n        \n        self.q_table[state, action] = new_q\n    \n    def decay_epsilon(self):\n        \"\"\"Decay exploration rate\"\"\"\n        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 3. Create Environment and Train Agent\n\nWe'll use the FrozenLake environment from Gymnasium. The agent must navigate from start (S) to goal (G) while avoiding holes (H).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def train_q_learning(env, agent, n_episodes=10000, max_steps=100):\n    \"\"\"\n    Train Q-Learning agent\n    \n    Parameters:\n    -----------\n    env : gym.Env\n        Environment to train on\n    agent : QLearningAgent\n        Q-Learning agent\n    n_episodes : int\n        Number of episodes to train\n    max_steps : int\n        Maximum steps per episode\n        \n    Returns:\n    --------\n    episode_rewards : list\n        Rewards obtained in each episode\n    episode_lengths : list\n        Length of each episode\n    \"\"\"\n    episode_rewards = []\n    episode_lengths = []\n    \n    for episode in range(n_episodes):\n        state, _ = env.reset()\n        total_reward = 0\n        \n        for step in range(max_steps):\n            # Select and perform action\n            action = agent.select_action(state)\n            next_state, reward, terminated, truncated, _ = env.step(action)\n            done = terminated or truncated\n            \n            # Update Q-table\n            agent.update(state, action, reward, next_state, done)\n            \n            total_reward += reward\n            state = next_state\n            \n            if done:\n                break\n        \n        # Decay epsilon after each episode\n        agent.decay_epsilon()\n        \n        episode_rewards.append(total_reward)\n        episode_lengths.append(step + 1)\n        \n        # Print progress\n        if (episode + 1) % 1000 == 0:\n            avg_reward = np.mean(episode_rewards[-100:])\n            print(f\"Episode {episode + 1}/{n_episodes}, \"\n                  f\"Avg Reward (last 100): {avg_reward:.3f}, \"\n                  f\"Epsilon: {agent.epsilon:.3f}\")\n    \n    return episode_rewards, episode_lengths",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create FrozenLake environment\nenv = gym.make('FrozenLake-v1', is_slippery=True, render_mode=None)\n\n# Get environment dimensions\nn_states = env.observation_space.n\nn_actions = env.action_space.n\n\nprint(f\"Environment: FrozenLake-v1\")\nprint(f\"Number of states: {n_states}\")\nprint(f\"Number of actions: {n_actions}\")\n\n# Create Q-Learning agent\nagent = QLearningAgent(\n    n_states=n_states,\n    n_actions=n_actions,\n    learning_rate=0.1,\n    discount_factor=0.95,\n    epsilon=1.0,\n    epsilon_decay=0.995,\n    epsilon_min=0.01\n)\n\n# Train the agent\nprint(\"\\nTraining Q-Learning agent...\")\nepisode_rewards, episode_lengths = train_q_learning(\n    env, agent, n_episodes=10000, max_steps=100\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4. Visualize Training Results",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Calculate moving average for smoother visualization\ndef moving_average(data, window_size=100):\n    \"\"\"Calculate moving average\"\"\"\n    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n\n# Create subplots\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Plot 1: Episode Rewards\naxes[0, 0].plot(episode_rewards, alpha=0.3, label='Raw Rewards')\nif len(episode_rewards) > 100:\n    axes[0, 0].plot(moving_average(episode_rewards, 100), \n                    label='Moving Average (100 episodes)', linewidth=2)\naxes[0, 0].set_xlabel('Episode')\naxes[0, 0].set_ylabel('Total Reward')\naxes[0, 0].set_title('Episode Rewards Over Time')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Plot 2: Episode Lengths\naxes[0, 1].plot(episode_lengths, alpha=0.3, label='Raw Lengths')\nif len(episode_lengths) > 100:\n    axes[0, 1].plot(moving_average(episode_lengths, 100), \n                    label='Moving Average (100 episodes)', linewidth=2)\naxes[0, 1].set_xlabel('Episode')\naxes[0, 1].set_ylabel('Episode Length')\naxes[0, 1].set_title('Episode Lengths Over Time')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Plot 3: Success Rate\nwindow = 100\nsuccess_rate = []\nfor i in range(len(episode_rewards) - window + 1):\n    success_rate.append(np.mean([r > 0 for r in episode_rewards[i:i+window]]))\naxes[1, 0].plot(success_rate, linewidth=2)\naxes[1, 0].set_xlabel('Episode')\naxes[1, 0].set_ylabel('Success Rate')\naxes[1, 0].set_title(f'Success Rate (Rolling {window} episodes)')\naxes[1, 0].grid(True, alpha=0.3)\n\n# Plot 4: Q-Table Heatmap (sample of learned values)\nim = axes[1, 1].imshow(agent.q_table, cmap='viridis', aspect='auto')\naxes[1, 1].set_xlabel('Action')\naxes[1, 1].set_ylabel('State')\naxes[1, 1].set_title('Learned Q-Table Values')\nplt.colorbar(im, ax=axes[1, 1])\n\nplt.tight_layout()\nplt.show()\n\n# Print final statistics\nprint(f\"\\nTraining Complete!\")\nprint(f\"Final epsilon: {agent.epsilon:.4f}\")\nprint(f\"Average reward (last 100 episodes): {np.mean(episode_rewards[-100:]):.4f}\")\nprint(f\"Success rate (last 100 episodes): {np.mean([r > 0 for r in episode_rewards[-100:]]):.2%}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 5. Evaluate Learned Policy\n\nTest the learned policy on multiple episodes to measure final performance.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def evaluate_policy(env, agent, n_episodes=100, max_steps=100):\n    \"\"\"\n    Evaluate the learned policy\n    \n    Parameters:\n    -----------\n    env : gym.Env\n        Environment to evaluate on\n    agent : QLearningAgent\n        Trained Q-Learning agent\n    n_episodes : int\n        Number of evaluation episodes\n    max_steps : int\n        Maximum steps per episode\n        \n    Returns:\n    --------\n    results : dict\n        Evaluation metrics\n    \"\"\"\n    rewards = []\n    lengths = []\n    successes = []\n    \n    # Save current epsilon and set to 0 for pure exploitation\n    original_epsilon = agent.epsilon\n    agent.epsilon = 0.0\n    \n    for episode in range(n_episodes):\n        state, _ = env.reset()\n        total_reward = 0\n        \n        for step in range(max_steps):\n            action = agent.select_action(state)\n            next_state, reward, terminated, truncated, _ = env.step(action)\n            done = terminated or truncated\n            \n            total_reward += reward\n            state = next_state\n            \n            if done:\n                break\n        \n        rewards.append(total_reward)\n        lengths.append(step + 1)\n        successes.append(total_reward > 0)\n    \n    # Restore original epsilon\n    agent.epsilon = original_epsilon\n    \n    results = {\n        'mean_reward': np.mean(rewards),\n        'std_reward': np.std(rewards),\n        'mean_length': np.mean(lengths),\n        'success_rate': np.mean(successes)\n    }\n    \n    return results\n\n# Evaluate the trained agent\neval_results = evaluate_policy(env, agent, n_episodes=100)\n\nprint(\"=\" * 50)\nprint(\"EVALUATION RESULTS\")\nprint(\"=\" * 50)\nprint(f\"Mean Reward: {eval_results['mean_reward']:.4f} ± {eval_results['std_reward']:.4f}\")\nprint(f\"Mean Episode Length: {eval_results['mean_length']:.2f}\")\nprint(f\"Success Rate: {eval_results['success_rate']:.2%}\")\nprint(\"=\" * 50)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 6. Visualize Learned Policy\n\nDisplay the optimal action for each state based on the learned Q-table.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Extract optimal policy from Q-table\noptimal_policy = np.argmax(agent.q_table, axis=1)\n\n# Map action indices to arrow symbols for visualization\naction_symbols = {0: '←', 1: '↓', 2: '→', 3: '↑'}\n\n# Create policy visualization for 4x4 grid (FrozenLake)\ngrid_size = 4\npolicy_grid = optimal_policy.reshape(grid_size, grid_size)\n\n# Create figure\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Plot 1: Policy as arrows\nfor i in range(grid_size):\n    for j in range(grid_size):\n        state = i * grid_size + j\n        action = optimal_policy[state]\n        axes[0].text(j, i, action_symbols[action], \n                    ha='center', va='center', fontsize=20)\naxes[0].set_xlim(-0.5, grid_size - 0.5)\naxes[0].set_ylim(-0.5, grid_size - 0.5)\naxes[0].set_xticks(range(grid_size))\naxes[0].set_yticks(range(grid_size))\naxes[0].grid(True)\naxes[0].invert_yaxis()\naxes[0].set_title('Learned Policy (Optimal Actions)', fontsize=14, fontweight='bold')\naxes[0].set_xlabel('Column')\naxes[0].set_ylabel('Row')\n\n# Plot 2: State Values (max Q-value for each state)\nstate_values = np.max(agent.q_table, axis=1).reshape(grid_size, grid_size)\nim = axes[1].imshow(state_values, cmap='RdYlGn', aspect='auto')\naxes[1].set_title('State Values (Max Q-value)', fontsize=14, fontweight='bold')\naxes[1].set_xlabel('Column')\naxes[1].set_ylabel('Row')\naxes[1].set_xticks(range(grid_size))\naxes[1].set_yticks(range(grid_size))\n\n# Add values as text\nfor i in range(grid_size):\n    for j in range(grid_size):\n        text = axes[1].text(j, i, f'{state_values[i, j]:.3f}',\n                           ha='center', va='center', color='black', fontsize=10)\n\nplt.colorbar(im, ax=axes[1])\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nLearned Policy Summary:\")\nprint(\"=\" * 50)\nprint(f\"Total states: {n_states}\")\nprint(f\"Actions: {['LEFT', 'DOWN', 'RIGHT', 'UP']}\")\nprint(f\"Optimal policy shape: {optimal_policy.shape}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}