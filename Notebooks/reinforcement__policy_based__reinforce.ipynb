{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REINFORCE Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Principle:** Direct Policy Gradient Optimization\n\n",
    "**Definition:** Directly optimizes policy parameters using gradient of expected reward (REINFORCE).\n\n",
    "**Algorithm Description:** REINFORCE directly parameterizes the policy as a neural network and uses Monte Carlo samples of entire episode trajectories to estimate policy gradients. It updates policy parameters in directions that increase the probability of actions leading to higher rewards, learning stochastic policies without value functions.\n\n",
    "**Typical Use Cases:**\n- Directly learning a stochastic policy\n- Foundation for policy gradient methods.\n- Model-free, on-policy algorithm\n- Updates policy parameters to favor high-reward trajectories\n- Works with continuous action spaces\n\n",
    "**Assumptions:**\n- Continuous/discrete actions\n- Full episode trajectories\n- High variance\n- On-policy learning\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Import Libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. REINFORCE Algorithm\n",
    "\n",
    "REINFORCE is a policy gradient method that directly optimizes the policy:\n",
    "\n",
    "**Policy Gradient Theorem:**\n",
    "\u2207J(\u03b8) = E[\u2207log \u03c0(a|s) * G_t]\n",
    "\n",
    "Where:\n",
    "- \u03b8 = policy parameters\n",
    "- \u03c0(a|s) = policy probability\n",
    "- G_t = discounted return from time t\n",
    "\n",
    "**Key idea:** Increase probability of actions that led to high returns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"Policy network that outputs action probabilities\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.softmax(self.fc3(x), dim=-1)  # Action probabilities"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class REINFORCEAgent:\n",
    "    \"\"\"REINFORCE agent with policy gradient\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, learning_rate=0.001, gamma=0.99):\n",
    "        self.gamma = gamma\n",
    "        self.policy = PolicyNetwork(state_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Storage for episode\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"Sample action from policy\"\"\"\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        probs = self.policy(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        \n",
    "        # Save log probability for later\n",
    "        self.saved_log_probs.append(m.log_prob(action))\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"Update policy using REINFORCE\"\"\"\n",
    "        # Calculate discounted returns\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(self.rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        \n",
    "        # Normalize returns for stability\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
    "        \n",
    "        # Calculate policy loss\n",
    "        policy_loss = []\n",
    "        for log_prob, G in zip(self.saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * G)  # Negative for gradient ascent\n",
    "        \n",
    "        # Update policy\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss = torch.stack(policy_loss).sum()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Clear episode data\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        \n",
    "        return policy_loss.item()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Train REINFORCE Agent"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def train_reinforce(env, agent, n_episodes=1000):\n",
    "    \"\"\"Train REINFORCE agent\"\"\"\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        \n",
    "        for t in range(500):  # Max steps\n",
    "            action = agent.select_action(state)\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            agent.rewards.append(reward)\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        # Update policy after episode\n",
    "        loss = agent.update()\n",
    "        \n",
    "        episode_rewards.append(sum(agent.rewards) if not agent.rewards else sum(episode_rewards[-1:]))\n",
    "        episode_lengths.append(t + 1)\n",
    "        \n",
    "        if (episode + 1) % 50 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-50:])\n",
    "            print(f\"Episode {episode + 1}/{n_episodes}, Avg Reward: {avg_reward:.2f}\")\n",
    "    \n",
    "    return episode_rewards, episode_lengths"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Create environment\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "print(f\"Environment: CartPole-v1\")\n",
    "print(f\"State dimension: {state_dim}\")\n",
    "print(f\"Action dimension: {action_dim}\")\n",
    "\n",
    "# Create REINFORCE agent\n",
    "agent = REINFORCEAgent(state_dim, action_dim, learning_rate=0.001, gamma=0.99)\n",
    "\n",
    "print(\"\\nTraining REINFORCE agent...\")\n",
    "episode_rewards, episode_lengths = train_reinforce(env, agent, n_episodes=1000)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Visualize Training Results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def moving_average(data, window=20):\n",
    "    return np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Episode Rewards\n",
    "axes[0].plot(episode_rewards, alpha=0.3, label='Raw Rewards')\n",
    "if len(episode_rewards) > 20:\n",
    "    axes[0].plot(moving_average(episode_rewards, 20), linewidth=2, label='MA(20)')\n",
    "axes[0].axhline(y=195, color='r', linestyle='--', label='Solved (195)')\n",
    "axes[0].set_xlabel('Episode')\n",
    "axes[0].set_ylabel('Total Reward')\n",
    "axes[0].set_title('REINFORCE: Episode Rewards')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Episode Lengths\n",
    "axes[1].plot(episode_lengths, alpha=0.3)\n",
    "if len(episode_lengths) > 20:\n",
    "    axes[1].plot(moving_average(episode_lengths, 20), linewidth=2, label='MA(20)')\n",
    "axes[1].set_xlabel('Episode')\n",
    "axes[1].set_ylabel('Episode Length')\n",
    "axes[1].set_title('Episode Lengths')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAverage reward (last 100 episodes): {np.mean(episode_rewards[-100:]):.2f}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5. Evaluate Policy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate_policy(env, agent, n_episodes=100):\n",
    "    rewards = []\n",
    "    for _ in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        for _ in range(500):\n",
    "            action = agent.select_action(state)\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        rewards.append(total_reward)\n",
    "    return np.mean(rewards), np.std(rewards)\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(env, agent, 100)\n",
    "print(\"=\" * 50)\n",
    "print(\"REINFORCE EVALUATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Mean Reward: {mean_reward:.2f} \u00b1 {std_reward:.2f}\")\n",
    "print(\"=\" * 50)\n",
    "env.close()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}