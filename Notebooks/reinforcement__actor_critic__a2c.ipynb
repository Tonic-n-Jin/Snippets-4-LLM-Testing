{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage Actor-Critic (A2C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principle:** Dual Critic-Actor Learning\n",
    "\n",
    "**Definition:** Combines policy gradient (actor) with learned value function (critic) to reduce variance (A2C).\n",
    "\n",
    "**Algorithm Description:** Advantage Actor-Critic uses two neural networks: an actor that learns the policy and a critic that learns a value function to estimate advantages (how much better an action is than average). The critic's advantage estimates reduce variance in policy gradient updates, stabilizing and accelerating learning.\n",
    "\n",
    "**Typical Use Cases:**\n",
    "- Combines policy-based (actor) and value-based (critic) methods\n",
    "- Synchronous version of a3c\n",
    "- Variance reduction in policy gradients\n",
    "- Works with continuous and discrete action spaces\n",
    "\n",
    "**Assumptions:**\n",
    "- Continuous/discrete actions\n",
    "- Large datasets\n",
    "- On-policy learning\n",
    "- Variance reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### âš™ï¸ Prerequisites & Setup\n\n**Required Packages:**\n- `gymnasium>=0.29.0` - RL environment\n- `torch>=2.0.0` - Deep learning framework\n- `numpy`, `matplotlib`, `seaborn` - Data processing and visualization\n\n**Installation:**\n```bash\npip install gymnasium torch numpy matplotlib seaborn\n```\n\n**â±ï¸ Estimated Training Time:**\n- CPU: ~10-15 minutes\n- GPU: ~4-6 minutes\n\n**ðŸ’¡ Quick Test:** To reduce training time, change `n_episodes=500` to `n_episodes=100` in the training cell.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. A2C (Advantage Actor-Critic)\n",
    "\n",
    "A2C combines value-based and policy-based methods:\n",
    "\n",
    "**Actor:** Updates policy using advantage\n",
    "**Critic:** Estimates state value V(s)\n",
    "\n",
    "**Advantage:** A(s,a) = Q(s,a) - V(s) = r + Î³V(s') - V(s)\n",
    "\n",
    "**Actor Loss:** -log Ï€(a|s) * A(s,a)\n",
    "**Critic Loss:** (V(s) - target)Â²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CNetwork(nn.Module):\n",
    "    \"\"\"Shared network for actor and critic\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden=128):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Linear(hidden, action_dim)\n",
    "        self.critic = nn.Linear(hidden, 1)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = self.shared(state)\n",
    "        return F.softmax(self.actor(x), dim=-1), self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class A2CAgent:\n    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99):\n        self.gamma = gamma\n        self.network = A2CNetwork(state_dim, action_dim)\n        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n    \n    def select_action(self, state):\n        state = torch.FloatTensor(state)\n        with torch.no_grad():\n            probs, _ = self.network(state)\n            dist = Categorical(probs)\n            action = dist.sample()\n        return action.item()\n    \n    def update(self, states, actions, rewards, next_states, dones):\n        states = torch.FloatTensor(states)\n        actions = torch.LongTensor(actions)\n        rewards = torch.FloatTensor(rewards)\n        next_states = torch.FloatTensor(next_states)\n        dones = torch.FloatTensor(dones)\n        \n        # Current policy and values\n        probs, values = self.network(states)\n        dist = Categorical(probs)\n        log_probs = dist.log_prob(actions)\n        \n        # Compute targets and advantages\n        with torch.no_grad():\n            _, next_values = self.network(next_states)\n            targets = rewards + self.gamma * next_values.squeeze() * (1 - dones)\n            advantages = targets - values.squeeze()\n        \n        # Actor loss (policy gradient)\n        actor_loss = -(log_probs * advantages).mean()\n        \n        # Critic loss (value function)\n        critic_loss = F.mse_loss(values.squeeze(), targets)\n        \n        # Entropy bonus for exploration\n        entropy = dist.entropy().mean()\n        \n        # Total loss\n        loss = actor_loss + 0.5 * critic_loss - 0.01 * entropy\n        \n        self.optimizer.zero_grad()\n        loss.backward()\n        \n        # Gradient clipping for training stability\n        torch.nn.utils.clip_grad_norm_(self.network.parameters(), max_norm=0.5)\n        \n        self.optimizer.step()\n        \n        return loss.item()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train A2C Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "agent = A2CAgent(env.observation_space.shape[0], env.action_space.n)\n",
    "\n",
    "episode_rewards = []\n",
    "batch_size = 5\n",
    "\n",
    "states_batch, actions_batch, rewards_batch = [], [], []\n",
    "next_states_batch, dones_batch = [], []\n",
    "\n",
    "print('Training A2C...')\n",
    "for episode in range(500):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for t in range(500):\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        states_batch.append(state)\n",
    "        actions_batch.append(action)\n",
    "        rewards_batch.append(reward)\n",
    "        next_states_batch.append(next_state)\n",
    "        dones_batch.append(done)\n",
    "        \n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        \n",
    "        # Update every batch_size steps\n",
    "        if len(states_batch) >= batch_size:\n",
    "            agent.update(states_batch, actions_batch, rewards_batch, \n",
    "                        next_states_batch, dones_batch)\n",
    "            states_batch, actions_batch, rewards_batch = [], [], []\n",
    "            next_states_batch, dones_batch = [], []\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    episode_rewards.append(total_reward)\n",
    "    \n",
    "    if (episode + 1) % 50 == 0:\n",
    "        print(f'Episode {episode+1}, Avg Reward: {np.mean(episode_rewards[-50:]):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(episode_rewards, alpha=0.3, label='Raw')\n",
    "if len(episode_rewards) > 20:\n",
    "    ma = np.convolve(episode_rewards, np.ones(20)/20, mode='valid')\n",
    "    plt.plot(ma, linewidth=2, label='MA(20)')\n",
    "plt.axhline(195, color='r', linestyle='--', label='Solved')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('A2C Training Performance')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nFinal Performance: {np.mean(episode_rewards[-100:]):.2f}')\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}