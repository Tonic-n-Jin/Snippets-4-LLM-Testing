{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Predictive Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Principle:** Predictive Control Optimization\n\n",
    "**Definition:** Optimizes a sequence of control actions over a predicted horizon subject to dynamics constraints (MPC).\n\n",
    "**Algorithm Description:** Model Predictive Control uses a known or learned model to predict future states over a finite horizon, optimizes a control sequence that maximizes predicted rewards, executes only the first control action, then re-plans at the next time step. This receding horizon approach handles constraints and disturbances effectively.\n\n",
    "**Typical Use Cases:**\n- Can handle constraints on states and actions.\n- Optimal control for dynamic systems (robotics, autonomous vehicles)\n- Repeatedly plans over a short horizon\n- When a model of the environment is available\n\n",
    "**Assumptions:**\n- Accurate environment model\n- Computational resources\n- Continuous states/actions\n- Dynamic system control\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Import Libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gymnasium as gym\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "np.random.seed(42)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Model Predictive Control (MPC)\n",
    "\n",
    "MPC optimizes actions over a future horizon using a learned model:\n",
    "\n",
    "**Algorithm:**\n",
    "1. **Learn Model:** Build model of environment dynamics\n",
    "2. **Plan:** Optimize action sequence to maximize predicted rewards\n",
    "3. **Execute:** Apply first action of optimal sequence\n",
    "4. **Replan:** Repeat at next time step (receding horizon)\n",
    "\n",
    "**Objective:** max \u03a3 r_t over horizon H\n",
    "\n",
    "**Note:** Simplified version using random shooting for CartPole"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class SimpleDynamicsModel:\n",
    "    \"\"\"Simple neural network model of environment dynamics\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # Store transitions for simple model\n",
    "        self.transitions = []\n",
    "    \n",
    "    def add_transition(self, state, action, next_state, reward):\n",
    "        \"\"\"Store transition\"\"\"\n",
    "        self.transitions.append((state, action, next_state, reward))\n",
    "    \n",
    "    def predict(self, state, action):\n",
    "        \"\"\"Predict next state and reward (simplified)\"\"\"\n",
    "        # Simple nearest neighbor prediction\n",
    "        if not self.transitions:\n",
    "            return state, 0.0\n",
    "        \n",
    "        # Find similar transitions\n",
    "        similar = [(s, a, ns, r) for s, a, ns, r in self.transitions \n",
    "                   if a == action]\n",
    "        \n",
    "        if not similar:\n",
    "            return state, 0.0\n",
    "        \n",
    "        # Average predictions\n",
    "        next_states = np.array([ns for _, _, ns, _ in similar])\n",
    "        rewards = np.array([r for _, _, _, r in similar])\n",
    "        \n",
    "        return next_states.mean(axis=0), rewards.mean()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class MPCAgent:\n",
    "    \"\"\"MPC agent with random shooting optimization\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, horizon=10, n_samples=100):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.horizon = horizon  # Planning horizon\n",
    "        self.n_samples = n_samples  # Number of action sequences to try\n",
    "        \n",
    "        self.model = SimpleDynamicsModel(state_dim, action_dim)\n",
    "    \n",
    "    def rollout(self, state, actions):\n",
    "        \"\"\"Simulate rollout using learned model\"\"\"\n",
    "        total_reward = 0\n",
    "        current_state = state.copy()\n",
    "        \n",
    "        for action in actions:\n",
    "            next_state, reward = self.model.predict(current_state, action)\n",
    "            total_reward += reward\n",
    "            current_state = next_state\n",
    "        \n",
    "        return total_reward\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"Select action using random shooting MPC\"\"\"\n",
    "        best_reward = -float('inf')\n",
    "        best_action = 0\n",
    "        \n",
    "        # Random shooting: try many random action sequences\n",
    "        for _ in range(self.n_samples):\n",
    "            # Sample random action sequence\n",
    "            actions = [np.random.randint(self.action_dim) \n",
    "                      for _ in range(self.horizon)]\n",
    "            \n",
    "            # Evaluate using model\n",
    "            predicted_reward = self.rollout(state, actions)\n",
    "            \n",
    "            # Keep best sequence\n",
    "            if predicted_reward > best_reward:\n",
    "                best_reward = predicted_reward\n",
    "                best_action = actions[0]  # Only use first action\n",
    "        \n",
    "        return best_action\n",
    "    \n",
    "    def update_model(self, state, action, next_state, reward):\n",
    "        \"\"\"Update dynamics model\"\"\"\n",
    "        self.model.add_transition(state, action, next_state, reward)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Train MPC Agent"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def train_mpc(env, agent, n_episodes=200):\n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(500):\n",
    "            # Select action using MPC (random if model empty)\n",
    "            if len(agent.model.transitions) < 100:\n",
    "                action = env.action_space.sample()  # Random exploration\n",
    "            else:\n",
    "                action = agent.select_action(state)\n",
    "            \n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Update model\n",
    "            agent.update_model(state, action, next_state, reward)\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        \n",
    "        if (episode + 1) % 50 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-50:])\n",
    "            print(f'Episode {episode+1}, Avg Reward: {avg_reward:.2f}, '\n",
    "                  f'Model size: {len(agent.model.transitions)}')\n",
    "    \n",
    "    return episode_rewards\n",
    "\n",
    "# Create environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Create MPC agent\n",
    "agent = MPCAgent(\n",
    "    state_dim=env.observation_space.shape[0],\n",
    "    action_dim=env.action_space.n,\n",
    "    horizon=5,  # Plan 5 steps ahead\n",
    "    n_samples=50  # Try 50 random sequences\n",
    ")\n",
    "\n",
    "print('Training MPC agent...')\n",
    "print('First 100 episodes: Random exploration to build model')\n",
    "print('After: MPC planning with learned model\\n')\n",
    "episode_rewards = train_mpc(env, agent, n_episodes=200)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Visualize Results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Episode Rewards\n",
    "axes[0].plot(episode_rewards, alpha=0.3, label='Raw')\n",
    "if len(episode_rewards) > 10:\n",
    "    ma = np.convolve(episode_rewards, np.ones(10)/10, mode='valid')\n",
    "    axes[0].plot(ma, linewidth=2, label='MA(10)')\n",
    "axes[0].axhline(195, color='r', linestyle='--', label='Solved')\n",
    "axes[0].axvline(100, color='gray', linestyle=':', label='MPC starts', alpha=0.5)\n",
    "axes[0].set_xlabel('Episode')\n",
    "axes[0].set_ylabel('Reward')\n",
    "axes[0].set_title('MPC Training Performance')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Model Growth\n",
    "model_sizes = list(range(1, len(agent.model.transitions) + 1))\n",
    "axes[1].plot(model_sizes, linewidth=2, color='purple')\n",
    "axes[1].set_xlabel('Transition')\n",
    "axes[1].set_ylabel('Model Size')\n",
    "axes[1].set_title('Dynamics Model Growth')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nFinal Performance: {np.mean(episode_rewards[-50:]):.2f}')\n",
    "print(f'Model learned from {len(agent.model.transitions)} transitions')\n",
    "env.close()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}