{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### A3C-Style Actor-Critic (Synchronous Implementation)\n\n**Note:** This is a simplified synchronous implementation inspired by A3C architecture. True A3C uses asynchronous parallel workers updating a shared global model."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": "**Principle:** Multi-Worker Parallel Learning (Synchronous Variant)\n\n**Definition:** Runs multiple agents in parallel, synchronously updating a shared model for diverse experience collection.\n\n**Algorithm Description:** This notebook implements a synchronous variant of A3C (Asynchronous Advantage Actor-Critic). While true A3C runs multiple actor-learner agents asynchronously on separate threads/processes, this implementation collects experience from multiple parallel environments synchronously and performs batched updates. This approach still benefits from diverse parallel exploration while being simpler to implement and debug.\n\n**Typical Use Cases:**\n- Actor-critic method with parallel environment exploration\n- Synchronous updates to a global model (simpler than true async)\n- Educational demonstration of multi-worker training\n- Works with continuous and discrete action spaces\n\n**Assumptions:**\n- Continuous/discrete actions\n- Large datasets\n- On-policy learning\n- Parallel environments (synchronous updates)"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Import Libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. A3C (Asynchronous Advantage Actor-Critic)\n",
    "\n",
    "A3C uses multiple parallel workers to collect experience:\n",
    "\n",
    "**Key Features:**\n",
    "1. **Multiple Workers:** Each worker has own environment instance\n",
    "2. **Asynchronous Updates:** Workers update global network independently\n",
    "3. **Shared Network:** All workers share same global parameters\n",
    "\n",
    "**Advantage:** Parallel exploration improves sample efficiency\n",
    "\n",
    "**Note:** This is a simplified synchronous version for demonstration"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class A3CNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Linear(128, action_dim)\n",
    "        self.critic = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return F.softmax(self.actor(x), dim=-1), self.critic(x)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class A3CAgent:\n    \"\"\"Simplified A3C (synchronous version)\"\"\"\n    \n    def __init__(self, state_dim, action_dim, n_workers=4, lr=3e-4, gamma=0.99):\n        self.gamma = gamma\n        self.n_workers = n_workers\n        \n        # Global shared network\n        self.global_network = A3CNetwork(state_dim, action_dim)\n        self.global_network.share_memory()  # Share across processes\n        self.optimizer = optim.Adam(self.global_network.parameters(), lr=lr)\n        \n        # Create multiple environments (workers)\n        self.envs = [gym.make('CartPole-v1') for _ in range(n_workers)]\n    \n    def worker_rollout(self, env, steps=5):\n        \"\"\"Collect experience from one worker\"\"\"\n        states, actions, rewards, next_states, dones = [], [], [], [], []\n        state, _ = env.reset()\n        \n        for _ in range(steps):\n            state_t = torch.FloatTensor(state)\n            with torch.no_grad():\n                probs, _ = self.global_network(state_t)\n                action = Categorical(probs).sample().item()\n            \n            next_state, reward, terminated, truncated, _ = env.step(action)\n            done = terminated or truncated\n            \n            states.append(state)\n            actions.append(action)\n            rewards.append(reward)\n            next_states.append(next_state)\n            dones.append(done)\n            \n            state = next_state\n            if done:\n                state, _ = env.reset()\n        \n        return states, actions, rewards, next_states, dones\n    \n    def update(self):\n        \"\"\"Collect experience from all workers and update\"\"\"\n        all_states, all_actions, all_rewards = [], [], []\n        all_next_states, all_dones = [], []\n        \n        # Collect from all workers\n        for env in self.envs:\n            s, a, r, ns, d = self.worker_rollout(env, steps=5)\n            all_states.extend(s)\n            all_actions.extend(a)\n            all_rewards.extend(r)\n            all_next_states.extend(ns)\n            all_dones.extend(d)\n        \n        # Convert to tensors\n        states = torch.FloatTensor(all_states)\n        actions = torch.LongTensor(all_actions)\n        rewards = torch.FloatTensor(all_rewards)\n        next_states = torch.FloatTensor(all_next_states)\n        dones = torch.FloatTensor(all_dones)\n        \n        # Compute loss\n        probs, values = self.global_network(states)\n        dist = Categorical(probs)\n        log_probs = dist.log_prob(actions)\n        \n        with torch.no_grad():\n            _, next_values = self.global_network(next_states)\n            targets = rewards + self.gamma * next_values.squeeze() * (1 - dones)\n            advantages = targets - values.squeeze()\n        \n        actor_loss = -(log_probs * advantages).mean()\n        critic_loss = F.mse_loss(values.squeeze(), targets)\n        entropy = dist.entropy().mean()\n        \n        loss = actor_loss + 0.5 * critic_loss - 0.01 * entropy\n        \n        self.optimizer.zero_grad()\n        loss.backward()\n        \n        # Gradient clipping for training stability\n        torch.nn.utils.clip_grad_norm_(self.global_network.parameters(), max_norm=0.5)\n        \n        self.optimizer.step()\n        \n        return loss.item()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Train A3C Agent"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "state_dim = 4  # CartPole\n",
    "action_dim = 2\n",
    "agent = A3CAgent(state_dim, action_dim, n_workers=4)\n",
    "\n",
    "# Evaluation environment\n",
    "eval_env = gym.make('CartPole-v1')\n",
    "episode_rewards = []\n",
    "\n",
    "print('Training A3C with 4 workers...')\n",
    "for episode in range(500):\n",
    "    # Update from all workers\n",
    "    agent.update()\n",
    "    \n",
    "    # Evaluate every 10 episodes\n",
    "    if episode % 10 == 0:\n",
    "        state, _ = eval_env.reset()\n",
    "        total_reward = 0\n",
    "        for _ in range(500):\n",
    "            with torch.no_grad():\n",
    "                probs, _ = agent.global_network(torch.FloatTensor(state))\n",
    "                action = probs.argmax().item()\n",
    "            state, reward, terminated, truncated, _ = eval_env.step(action)\n",
    "            total_reward += reward\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        episode_rewards.append(total_reward)\n",
    "    \n",
    "    if (episode + 1) % 50 == 0:\n",
    "        print(f'Episode {episode+1}, Avg Eval Reward: {np.mean(episode_rewards[-5:]):.2f}')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Visualize Results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(episode_rewards, marker='o', linewidth=2, label='Eval Reward')\n",
    "plt.axhline(195, color='r', linestyle='--', label='Solved')\n",
    "plt.xlabel('Evaluation Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('A3C Training Performance (4 Workers)')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nFinal Performance: {np.mean(episode_rewards[-10:]):.2f}')\n",
    "eval_env.close()\n",
    "for env in agent.envs:\n",
    "    env.close()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}