{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Principle:** Deep Value Function Approximation\n\n",
    "**Definition:** Uses a neural network to approximate Q-values, trained via experience replay and target networks (DQN).\n\n",
    "**Algorithm Description:** Deep Q-Network approximates the Q-function using a deep neural network that takes states as input and outputs Q-values for each possible action. It uses experience replay (storing and randomly sampling past experiences) and target networks (periodically updated copies) to stabilize learning in high-dimensional spaces.\n\n",
    "**Typical Use Cases:**\n- Combines q-learning with neural networks\n- Famous for playing atari games from pixels.\n- Model-free, off-policy control\n- Solving problems with high-dimensional or continuous state spaces\n\n",
    "**Assumptions:**\n- Discrete actions\n- Experience replay\n- High-dimensional states\n- Large datasets\n- Neural network training\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### ‚öôÔ∏è Prerequisites & Setup\n\n**Required Packages:**\n- `gymnasium>=0.29.0` - RL environment\n- `torch>=2.0.0` - Deep learning framework\n- `numpy`, `matplotlib`, `seaborn` - Data processing and visualization\n\n**Installation:**\n```bash\npip install gymnasium torch numpy matplotlib seaborn\n```\n\n**‚è±Ô∏è Estimated Training Time:**\n- CPU: ~15-20 minutes\n- GPU: ~5-8 minutes\n\n**üí° Quick Test:** To reduce training time for testing, change `n_episodes=500` to `n_episodes=100` in the training cell.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Import Libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Deep Q-Network (DQN)\n",
    "\n",
    "DQN uses a neural network to approximate Q(s, a) instead of a table:\n",
    "\n",
    "**Key innovations:**\n",
    "1. **Experience Replay**: Store transitions in memory and sample randomly for training\n",
    "2. **Target Network**: Separate network for computing target Q-values (stabilizes training)\n",
    "3. **Neural Network**: Approximates Q-values for continuous/large state spaces\n",
    "\n",
    "**Loss Function:** \n",
    "L = (r + Œ≥ max Q_target(s', a') - Q(s, a))¬≤"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Neural network for approximating Q-values\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)  # Q-values for all actions"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Experience replay buffer\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Experience replay buffer for storing and sampling transitions\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append(Transition(state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class DQNAgent:\n    \"\"\"DQN agent with experience replay and target network\"\"\"\n    \n    def __init__(self, state_dim, action_dim, learning_rate=0.001, \n                 gamma=0.99, epsilon=1.0, epsilon_decay=0.995, \n                 epsilon_min=0.01, buffer_size=10000, batch_size=64):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.epsilon_decay = epsilon_decay\n        self.epsilon_min = epsilon_min\n        self.batch_size = batch_size\n        \n        # Q-network and target network\n        self.q_network = QNetwork(state_dim, action_dim)\n        self.target_network = QNetwork(state_dim, action_dim)\n        self.target_network.load_state_dict(self.q_network.state_dict())\n        \n        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n        self.replay_buffer = ReplayBuffer(buffer_size)\n    \n    def select_action(self, state):\n        \"\"\"Epsilon-greedy action selection\"\"\"\n        if np.random.random() < self.epsilon:\n            return np.random.randint(self.action_dim)\n        else:\n            with torch.no_grad():\n                state_tensor = torch.FloatTensor(state).unsqueeze(0)\n                q_values = self.q_network(state_tensor)\n                return q_values.argmax().item()\n    \n    def train_step(self):\n        \"\"\"Perform one training step using experience replay\"\"\"\n        if len(self.replay_buffer) < self.batch_size:\n            return None\n        \n        # Sample batch from replay buffer\n        transitions = self.replay_buffer.sample(self.batch_size)\n        batch = Transition(*zip(*transitions))\n        \n        # Convert to tensors\n        state_batch = torch.FloatTensor(batch.state)\n        action_batch = torch.LongTensor(batch.action)\n        reward_batch = torch.FloatTensor(batch.reward)\n        next_state_batch = torch.FloatTensor(batch.next_state)\n        done_batch = torch.FloatTensor(batch.done)\n        \n        # Current Q-values\n        current_q_values = self.q_network(state_batch).gather(1, action_batch.unsqueeze(1))\n        \n        # Target Q-values (using target network)\n        with torch.no_grad():\n            max_next_q_values = self.target_network(next_state_batch).max(1)[0]\n            target_q_values = reward_batch + (1 - done_batch) * self.gamma * max_next_q_values\n        \n        # Compute loss and update\n        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)\n        \n        self.optimizer.zero_grad()\n        loss.backward()\n        \n        # Gradient clipping for training stability\n        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=1.0)\n        \n        self.optimizer.step()\n        \n        return loss.item()\n    \n    def update_target_network(self):\n        \"\"\"Copy weights from Q-network to target network\"\"\"\n        self.target_network.load_state_dict(self.q_network.state_dict())\n    \n    def decay_epsilon(self):\n        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Train DQN Agent"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def train_dqn(env, agent, n_episodes=500, target_update_freq=10):\n",
    "    \"\"\"Train DQN agent\"\"\"\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    losses = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        episode_loss = []\n",
    "        \n",
    "        for step in range(500):  # Max steps per episode\n",
    "            # Select and perform action\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Store transition in replay buffer\n",
    "            agent.replay_buffer.push(state, action, reward, next_state, float(done))\n",
    "            \n",
    "            # Train on batch from replay buffer\n",
    "            loss = agent.train_step()\n",
    "            if loss is not None:\n",
    "                episode_loss.append(loss)\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Update target network periodically\n",
    "        if (episode + 1) % target_update_freq == 0:\n",
    "            agent.update_target_network()\n",
    "        \n",
    "        # Decay epsilon\n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(step + 1)\n",
    "        if episode_loss:\n",
    "            losses.append(np.mean(episode_loss))\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % 50 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-50:])\n",
    "            print(f\"Episode {episode + 1}/{n_episodes}, \"\n",
    "                  f\"Avg Reward: {avg_reward:.2f}, \"\n",
    "                  f\"Epsilon: {agent.epsilon:.3f}\")\n",
    "    \n",
    "    return episode_rewards, episode_lengths, losses"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Create CartPole environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "print(f\"Environment: CartPole-v1\")\n",
    "print(f\"State dimension: {state_dim}\")\n",
    "print(f\"Number of actions: {action_dim}\")\n",
    "\n",
    "# Create DQN agent\n",
    "agent = DQNAgent(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    learning_rate=0.001,\n",
    "    gamma=0.99,\n",
    "    epsilon=1.0,\n",
    "    epsilon_decay=0.995,\n",
    "    epsilon_min=0.01\n",
    ")\n",
    "\n",
    "print(\"\\nTraining DQN agent...\")\n",
    "episode_rewards, episode_lengths, losses = train_dqn(\n",
    "    env, agent, n_episodes=500, target_update_freq=10\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Visualize Training Results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def moving_average(data, window_size=20):\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Episode Rewards\n",
    "axes[0, 0].plot(episode_rewards, alpha=0.3, label='Raw Rewards')\n",
    "if len(episode_rewards) > 20:\n",
    "    axes[0, 0].plot(moving_average(episode_rewards, 20), \n",
    "                    label='Moving Average (20 episodes)', linewidth=2)\n",
    "axes[0, 0].axhline(y=195, color='r', linestyle='--', label='Solved Threshold (195)')\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Total Reward')\n",
    "axes[0, 0].set_title('DQN: Episode Rewards Over Time')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Episode Lengths\n",
    "axes[0, 1].plot(episode_lengths, alpha=0.3, label='Raw Lengths')\n",
    "if len(episode_lengths) > 20:\n",
    "    axes[0, 1].plot(moving_average(episode_lengths, 20), \n",
    "                    label='Moving Average (20 episodes)', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('Episode Length')\n",
    "axes[0, 1].set_title('Episode Lengths Over Time')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Training Loss\n",
    "if losses:\n",
    "    axes[1, 0].plot(losses, alpha=0.5)\n",
    "    if len(losses) > 20:\n",
    "        axes[1, 0].plot(moving_average(losses, 20), linewidth=2, label='MA(20)')\n",
    "    axes[1, 0].set_xlabel('Episode')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].set_title('Training Loss')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Success Rate\n",
    "window = 50\n",
    "success_threshold = 195\n",
    "if len(episode_rewards) >= window:\n",
    "    success_rate = []\n",
    "    for i in range(len(episode_rewards) - window + 1):\n",
    "        success_rate.append(np.mean([r >= success_threshold \n",
    "                                     for r in episode_rewards[i:i+window]]))\n",
    "    axes[1, 1].plot(success_rate, linewidth=2, color='green')\n",
    "    axes[1, 1].set_xlabel('Episode')\n",
    "    axes[1, 1].set_ylabel('Success Rate')\n",
    "    axes[1, 1].set_title(f'Success Rate (Rolling {window} episodes)')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining Complete!\")\n",
    "print(f\"Final epsilon: {agent.epsilon:.4f}\")\n",
    "print(f\"Average reward (last 50 episodes): {np.mean(episode_rewards[-50:]):.2f}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5. Evaluate Learned Policy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate_dqn(env, agent, n_episodes=100):\n",
    "    \"\"\"Evaluate trained DQN agent\"\"\"\n",
    "    rewards = []\n",
    "    \n",
    "    # Save epsilon and set to 0 for pure exploitation\n",
    "    original_epsilon = agent.epsilon\n",
    "    agent.epsilon = 0.0\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for _ in range(500):\n",
    "            action = agent.select_action(state)\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "    \n",
    "    agent.epsilon = original_epsilon\n",
    "    \n",
    "    return {\n",
    "        'mean_reward': np.mean(rewards),\n",
    "        'std_reward': np.std(rewards),\n",
    "        'success_rate': np.mean([r >= 195 for r in rewards])\n",
    "    }\n",
    "\n",
    "eval_results = evaluate_dqn(env, agent, n_episodes=100)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"DQN EVALUATION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Mean Reward: {eval_results['mean_reward']:.2f} ¬± {eval_results['std_reward']:.2f}\")\n",
    "print(f\"Success Rate (>=195): {eval_results['success_rate']:.2%}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "env.close()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}