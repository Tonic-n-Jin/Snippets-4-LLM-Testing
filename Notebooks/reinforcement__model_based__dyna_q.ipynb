{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dyna-Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Principle:** Integrated Planning & Learning\n\n",
    "**Definition:** Learns a model of the environment and uses it to simulate transitions for planning (Dyna-Q).\n\n",
    "**Algorithm Description:** Dyna-Q combines model-free Q-Learning with model-based planning by learning both a Q-function and a model of environment dynamics. During learning, it performs real updates from actual experience and simulated updates by planning with the learned model, improving sample efficiency.\n\n",
    "**Typical Use Cases:**\n- Combines model-free (q-learning) with model-based (planning) updates\n- Improving sample efficiency when real interaction is costly\n- Learns a model of the environment to generate simulated experiences.\n\n",
    "**Assumptions:**\n- Discrete state/action spaces\n- Model learning\n- Sample efficiency\n- Simple environment dynamics\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Import Libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gymnasium as gym\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "np.random.seed(42)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Dyna-Q Algorithm\n",
    "\n",
    "Dyna-Q integrates planning and learning:\n",
    "\n",
    "**Key Components:**\n",
    "1. **Direct RL:** Learn from real experience (Q-Learning)\n",
    "2. **Model Learning:** Build model of environment dynamics\n",
    "3. **Planning:** Use model to generate simulated experience\n",
    "\n",
    "**Update Process:**\n",
    "1. Take real action, observe transition\n",
    "2. Update Q-table from real experience\n",
    "3. Update environment model\n",
    "4. Perform n planning steps using model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class DynaQAgent:\n",
    "    \"\"\"Dyna-Q agent with model-based planning\"\"\"\n",
    "    \n",
    "    def __init__(self, n_states, n_actions, learning_rate=0.1, \n",
    "                 discount_factor=0.95, epsilon=0.1, n_planning_steps=10):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.n_planning_steps = n_planning_steps\n",
    "        \n",
    "        # Q-table\n",
    "        self.q_table = np.zeros((n_states, n_actions))\n",
    "        \n",
    "        # Environment model: model[(s,a)] = (next_state, reward)\n",
    "        self.model = {}\n",
    "        \n",
    "        # Track visited state-action pairs for planning\n",
    "        self.visited_pairs = []\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"Epsilon-greedy action selection\"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])\n",
    "    \n",
    "    def q_learning_update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Q-Learning update from real or simulated experience\"\"\"\n",
    "        current_q = self.q_table[state, action]\n",
    "        max_next_q = 0 if done else np.max(self.q_table[next_state])\n",
    "        new_q = current_q + self.lr * (reward + self.gamma * max_next_q - current_q)\n",
    "        self.q_table[state, action] = new_q\n",
    "    \n",
    "    def update_model(self, state, action, reward, next_state):\n",
    "        \"\"\"Update environment model\"\"\"\n",
    "        self.model[(state, action)] = (next_state, reward)\n",
    "        if (state, action) not in self.visited_pairs:\n",
    "            self.visited_pairs.append((state, action))\n",
    "    \n",
    "    def planning(self):\n",
    "        \"\"\"Perform planning steps using learned model\"\"\"\n",
    "        for _ in range(self.n_planning_steps):\n",
    "            if not self.visited_pairs:\n",
    "                break\n",
    "            \n",
    "            # Sample random previously visited state-action pair\n",
    "            state, action = random.choice(self.visited_pairs)\n",
    "            \n",
    "            # Get predicted next state and reward from model\n",
    "            next_state, reward = self.model[(state, action)]\n",
    "            \n",
    "            # Update Q-table using simulated experience\n",
    "            self.q_learning_update(state, action, reward, next_state, False)\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Complete Dyna-Q step: learn + plan\"\"\"\n",
    "        # (a) Direct RL: Update Q from real experience\n",
    "        self.q_learning_update(state, action, reward, next_state, done)\n",
    "        \n",
    "        # (b) Model Learning: Update model\n",
    "        self.update_model(state, action, reward, next_state)\n",
    "        \n",
    "        # (c) Planning: Learn from simulated experience\n",
    "        self.planning()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Train Dyna-Q Agent"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def train_dyna_q(env, agent, n_episodes=300, max_steps=100):\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Dyna-Q step (learning + planning)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(step + 1)\n",
    "        \n",
    "        if (episode + 1) % 50 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-50:])\n",
    "            print(f'Episode {episode+1}, Avg Reward: {avg_reward:.3f}, '\n",
    "                  f'Model size: {len(agent.model)}')\n",
    "    \n",
    "    return episode_rewards, episode_lengths\n",
    "\n",
    "# Create environment\n",
    "env = gym.make('FrozenLake-v1', is_slippery=True, render_mode=None)\n",
    "\n",
    "# Create Dyna-Q agent\n",
    "agent = DynaQAgent(\n",
    "    n_states=env.observation_space.n,\n",
    "    n_actions=env.action_space.n,\n",
    "    learning_rate=0.1,\n",
    "    discount_factor=0.95,\n",
    "    epsilon=0.1,\n",
    "    n_planning_steps=50  # More planning steps\n",
    ")\n",
    "\n",
    "print('Training Dyna-Q with 50 planning steps per update...')\n",
    "episode_rewards, episode_lengths = train_dyna_q(env, agent, n_episodes=300)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Visualize Results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Episode Rewards\n",
    "axes[0].plot(episode_rewards, alpha=0.3, label='Raw')\n",
    "if len(episode_rewards) > 20:\n",
    "    ma = np.convolve(episode_rewards, np.ones(20)/20, mode='valid')\n",
    "    axes[0].plot(ma, linewidth=2, label='MA(20)')\n",
    "axes[0].set_xlabel('Episode')\n",
    "axes[0].set_ylabel('Reward')\n",
    "axes[0].set_title('Dyna-Q: Episode Rewards')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Success Rate\n",
    "window = 20\n",
    "success_rate = [np.mean([r > 0 for r in episode_rewards[i:i+window]]) \n",
    "                for i in range(len(episode_rewards) - window + 1)]\n",
    "axes[1].plot(success_rate, linewidth=2, color='green')\n",
    "axes[1].set_xlabel('Episode')\n",
    "axes[1].set_ylabel('Success Rate')\n",
    "axes[1].set_title(f'Success Rate (Rolling {window} episodes)')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nFinal Success Rate: {np.mean([r > 0 for r in episode_rewards[-100:]]):.2%}')\n",
    "print(f'Model learned {len(agent.model)} state-action transitions')\n",
    "env.close()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}