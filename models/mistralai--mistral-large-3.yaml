# Mistral Large 3
# Model defaults for mistralai/mistral-large-3
#
# Merge priority: CLI > per-step config > this file > global defaults

display_name: "Mistral Large 3"
description: "Mistral's flagship open-weight MoE model with 675B total / 41B active parameters"
context_length: 256000
output_length: 16384

# Recommended defaults for this model
temperature: 0.7
max_tokens: 4096
top_p: 1.0

# Model capabilities
capabilities:
  - mixture-of-experts (granular MoE)
  - long context (256K tokens)
  - multimodal understanding
  - code generation
  - function calling
  - multilingual

# Model specifications
parameters_total: 675000000000
parameters_active: 41000000000

# Strengths
strengths:
  - "#2 in OSS non-reasoning models on LMArena"
  - Apache 2.0 license (fully open source)
  - Trained from scratch on 3000 H200 GPUs
  - 80% lower price than previous generation
  - Available in FP8 and NVFP4 optimized variants

# Limitations
limitations:
  - Not a reasoning model (no extended thinking)
  - Large inference requirements

# Prompting notes
notes: |
  Mistral Large 3 offers frontier performance with open weights.
  Best for general-purpose tasks, code generation, and multilingual
  applications. Use granular MoE architecture for efficient inference.
