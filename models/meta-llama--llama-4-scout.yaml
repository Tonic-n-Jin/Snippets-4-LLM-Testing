capabilities: []
context_length: 327680
description: 'Llama 4 Scout 17B Instruct (16E) is a mixture-of-experts (MoE) language
  model developed by Meta, activating 17 billion parameters out of a total of 109B.
  It supports native multimodal input (text and image) and multilingual output (text
  and code) across 12 supported languages. Designed for assistant-style interaction
  and visual reasoning, Scout uses 16 experts per forward pass and features a context
  length of 10 million tokens, with a training corpus of ~40 trillion tokens.


  Built for high efficiency and local or commercial deployment, Llama 4 Scout incorporates
  early fusion for seamless modality integration. It is instruction-tuned for use
  in multilingual chat, captioning, and image understanding tasks. Released under
  the Llama 4 Community License, it was last trained on data up to August 2024 and
  launched publicly on April 5, 2025.'
display_name: 'Meta: Llama 4 Scout'
limitations: []
max_tokens: 16384
notes: ''
output_length: 16384
pricing_input: '0.00000008'
pricing_output: '0.0000003'
strengths: []
temperature: 1.0
top_p: 1.0
training_data_cutoff: null
